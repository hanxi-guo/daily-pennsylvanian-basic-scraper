# Daily Pennsylvanian Basic Scraper

## Overview
This repository contains a web scraper designed to extract news articles from **The Daily Pennsylvanian**. The scraper fetches the **main headline** and the **most recent article**, including their respective links, and stores them for further analysis.

This project was initially created from a [template](https://github.com/jlumbroso/basic-git-scraper-template)

---

## Changes & Improvements

### 1. Extracting Both Headlines & Links Together

Before: The scraper only stored **headlines**, making it harder to find original articles.

After: Now, the scraper formats link and  together for clarity, eg.

```
"2025-02-23 01:52PM",
"main_headline: Penn to reduce graduate admissions, rescind acceptances amid federal research funding cuts; Reference Link:  https://www.thedp.com/article/2025/02/penn-graduate-student-class-size-cut-trump-funding"
    
```

âœ… This keeps articles **self-contained** and easier to reference.

---

### 2. Improved Data Storage with `add_today` Function
Before: Headlines and links were stored as individual records, making it harder to associate them.

After: The `add_today` function now ensures each event entry includes **both the title and link** in a single structured format:

```python
add_today(value=headline, link=link, event_type="main_headline")
```

âœ…  This ensures data consistency and avoids unnecessary duplicate entries.

---

### 3. More Robust Error Handling
Before: If a request failed or a required element was missing, the scraper would crash.

After: Add the if-else block to prevent the program crashing if the most recent acticle do not found.

```python
if most_recent_article:
    ...
else:
   print("Most Recent article not found.")
```
âœ… This prevents failures from stopping the script and ensures errors are printed.

---

### 4. Adjusted Scheduling for New York Time and Run Twice a Day
Before: The scraper ran based on UTC time and run only one time every day.

After: The schedule now ensures execution at 9 AM and 4 PM EST time.
```yaml
schedule:
  - cron: "0 14,21 * * *"  # Runs at 9 AM & 4 PM EST time (UTC 14:00 & 21:00)
```

âœ…  Ensures reports are updated at the most relevant times for readers in Philadelphia, see details in [SCHEDULE-EXPLANATION.md](SCHEDULE-EXPLANATION.md).

---

## How the Scraper Works

1. Load the content from page
   - Uses *BeautifulSoup* for static pages.
   - Uses *requests* to make HTTP requests to scrape web pages

2. Extracts main headlines and recent articles
   - Finds elements using `soup.find()`.
   - Stores them along with their links.

3.  Stores data in a structured format using `add_today()`
   - Titles and links are combined into a single structured entry.

4. Runs automatically via GitHub Actions.
   - Scheduled to scrape twice a day via [scrape.yaml](.github/workflows/scrape.yaml) .

---

## Future Improvements
- **Automate daylight savings adjustments** : to avoid manual `cron` updates.
- **Optimize JavaScript-based scraping** : to scrape content automatically generated by JavaScript.

---

### Contributions & Feedback
If you have suggestions or encounter issues, feel free to open an issue or contribute to improvements!

---
## License

This project is licensed under the MIT License.
---

ðŸš€ **Happy Scraping!** 

